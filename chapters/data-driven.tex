\chapter{Data-driven approach leveraging explicit feedback as target variable on Customer Satisfaction}
\label{ch:dataDriven}
Instead of using identified relevant features from the first approach to make predictions about a satisfaction level of customers, this approach should be fed with more features and find its way to the promising ones automatically. The goal was to work on a software framework which analyses data and selects the influential factors for Customer Satisfaction automatically while throwing away garbage data and redundant features. Furthermore, the lack of metrics in the data representing Customer Satisfaction identified as major issue during the first part of the thesis should be tackled by getting explicit feedback from customers on how satisfied they are. The expectation from this customer survey is that it provides a much more reliable quantitative measurement of the satisfaction level of a customer than any other kind of data Tractive collects in its database system. As described in more detail in the satisfaction theory part of chapter \ref{ch:backgroundResearch} understanding why a customer behaves in a certain way can be quite subjective, involve psychological factors and vary among different types of customers. Therefore it has to be admitted that this can hardly be extracted from some objectively collected data and the actual source of truth is to ask people themselves. With a sufficient number of survey answers the framework should first analyse all observations retrieved by leveraging descriptive statistics and evaluate chances to predict satisfaction for arbitrary customers, who have not filled in the survey. With specific features selected machine learning algorithms can then be trained and evaluated on the data to find a model which is suited to do automated predictions. 

\section{Implementation of a survey to gather explicit feedback from customers}
Before any further data analysis task was started the initial task was to create a customer survey to collect usable information which gives a reliable insight into how a customer rates his experience with the product so far. A requirement was to keep the effort for the customer to fill in the survey low, which in first place means to require little time to fill in the duty part of the survey. Little time consumption correlates positively with response rate. According to \cite{reichheld2003one} customer surveys are often too complex and as a result overwhelming for the user even though the company cannot derive better decision from it. With regard to customer satisfaction and loyalty \cite{reichheld2003one} claim that the most essential number is the willingness of a customer to recommend the product to a friend or colleague. This metric should be on a scale of 0-10 to allow a computation of the NPS (Net-Promoter-Score), a widely used score to constitute growth of a company. In collaboration with responsible people at Tractive it was decided to limit the number of questions to the following two which were used later on to establish the target attribute in the prediction framework.

\begin{enumerate}
	\item How satisfied are you with Tractive GPS? (Scale 1-5)
	\item How likely are you to recommend Tractive to a friend or colleague? (Scale 0-10)
\end{enumerate}

Next to the specifications of questions, the survey should be appealing, personalized and easy to use. The final landing page of the customer survey is shown in figure \ref{fig:customerSurvey}. 

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{img/customerSurvey.png}
	\caption{Customer Survey - Landing page}
	\label{fig:customerSurvey}
\end{figure} 

The goal of the company was to get customer feedback early on to be able to respond to unsatisfied customers quickly and in best case before they complain about a problem they have at customer support. As a result, the author of the thesis could arrange with the responsible people of Tractive to following procedure. When a new user converts successfully to a paying customer, he or she has to activate the GPS device before first usage. This enables the device to be actively used, sending positions and showing up on the map in the apps. Directly after activating the device, the backend service schedules an email in two weeks asking the customer to fill in the survey. To reach as many users as possible the survey was translated into the five major languages supported at Tractive, namely German, English, French, Italian and Spanish. Depending on the users stored demographical settings the correct language version is triggered. An example of how the email looks like is shown in figure \ref{fig:customerSurveyEmail}.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{img/customerSurveyEmail.png}
	\caption{Customer Survey - Email}
	\label{fig:customerSurveyEmail}
\end{figure} 

When the user clicks on the "Take Survey" button in the email, a browser window will open with the customer survey landing page. Behind the scenes In the URL (Uniform Resource Locator), the language version of the survey as well as the name of the customer is passed. For better personalization of the voluntary additional questions the name of the pet associated with the GPS device is sent as URL parameter. A necessary information is the unique id of the device since it allows to query all of the identified hardware- and position related data identified in section \ref{sec:dataSources}. For designing the customer survey the external tool Typeform was used. The following section describes briefly how the data gets extracted and integrated back into the internal database to be processable. Furthermore a few statistical details will be pointed out to get an impression with regard to the pending satisfaction analysis and prediction task.

\section{Results and interpretation of survey results}
\label{sec:surveyResults}
To fetch the survey results from Typeform, a nightly job was implemented which uses the provided API of Typeform and gets the results from the previous day. The essential metrics from the two duty questions are extracted and along with the user- , pet- and device data stored in a new collection in the main database of Tractive. A typical structure of a document in this collection is illustrated in table \ref{tab:surveyResponse}.

\begin{table}[]
	\centering
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{|l|l|l|}
			\hline
			\multicolumn{1}{|c|}{\textbf{Attribute}} & \multicolumn{1}{c|}{\textbf{Datatype}} & \multicolumn{1}{c|}{\textbf{Description}} \\ \hline
			\_id & ObjectId & Unique identifier of document \\ \hline
			survey\_id & String & Typeform identifier for the particular language version \\ \hline
			submit\_date & DateTime & Date and time when customer submitted survey \\ \hline
			user\_id & ObjectId & Reference to the user \\ \hline
			tracker\_id & ObjectId & Reference to the device \\ \hline
			rating & Integer & Overall satisfaction (scale: 1-5) \\ \hline
			recommendation\_score & Integer & Recommendation potential (scale: 0-10) \\ \hline
		\end{tabular}%
	}
	\caption{Structure of a survey response represented in the company database}
	\label{tab:surveyResponse}
\end{table}

The rating and recommendation score are the two interesting numbers when it comes to predicting satisfaction for an arbitrary user afterwards. The customer survey was deployed to the productive environment on 03.07.2017 which yielded the first survey result submitted on 17.07.2017. As of 28.10.2017 17:42 following statistics regarding the survey could be extracted.

\begin{itemize}
	\item Number of customer survey emails sent: 15695
	\item Number of customers who filled in the survey: 2182
	\item Percentage of users who filled in the survey: 13.9\%
\end{itemize}

As these numbers show, the response rate of the customer survey fortunately is quite good. The collected amount of customer survey data so far should be sufficient for finding potential patterns in the data. In order to get a better understanding some descriptive statistic values for the satisfaction rating and the value indicating willigness of a customer to recommend Tractive were calculated. The results are shown in table \ref{tab:statisticDescriptive}. 

\begin{table}[]
	\centering
	\resizebox{\columnwidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			\textbf{Survey metric} & \textbf{Min.} & \textbf{1. Quartile} & \multicolumn{1}{l|}{\textbf{Median}} & \multicolumn{1}{l|}{\textbf{Mean}} & \multicolumn{1}{l|}{\textbf{3. Quartile}} & \multicolumn{1}{l|}{\textbf{Max.}} & \multicolumn{1}{l|}{\textbf{Variance}} & \multicolumn{1}{l|}{\textbf{Standard dev.}} \\ \hline
			Satisfaction & 1 & 3 & 4 & 3.688 & 5 & 5 & 1.327 & 1.152 \\ \hline
			Recommendation & 0 & 6 & 8 & 7.137 & 9 & 10 & 7.574 & 2.752 \\ \hline
		\end{tabular} %
	}
	\caption{Statistical summary - Overall satisfaction and recommendation score}
	\label{tab:statisticDescriptive}
\end{table}

Based on the results it can be followed that the average customer rates his or her satisfaction with the Tractive GPS product as mostly satisfied represented by a value close to 4. Similar is the recommendation likeliness value where the average lies between 7 and 8. Since the first quartile with a value of 6 is rather high, it can be followed that 75\% of the customers are more likely to recommend Tractive to a friend or colleague. To close this statistical analysis of survey responses it can be stated that the majority of customers tend to be satisfied which had to be considered accordingly in the prediction framework outlined in more detail in the following section \ref{sec:predictionFramework}.

\section{Software architecture and implementation of prediction framework}
\label{sec:predictionFramework}
This chapter provides detailed insights into the architecture and implementation of a Customer Satisfaction prediction framework leveraging introduced data mining techniques from section \ref{ssec:bottomUp}. Based on the data extracted by the implemented tool outlined in the previous chapter, it was the goal to train a suitable machine learning model on this data of users who answered the customer survey and increase the certainty of satisfaction outcome. As common in nearly any machine learning task, intensive experimenting in order to achieve good results is inevitable. Therefore the author tried to automate those experiments. Before elaborating on the individual components of the prediction framework, figure \ref{fig:frameworkOverview} provides a broad overview.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{img/frameworkOverview.png}
	\caption{Overview on the prediction framework}
	\label{fig:frameworkOverview}
\end{figure} 

\subsection{Preparing training data}
As the relevant data to analyze has already been queried with the implemented data extraction tool introduced in the last chapter, big parts of the implementation could be reused to prepare the training data for the prediction framework. However in contrast to the selection of random samples, it was necessary to limit the selection of the data to customers who had filled in the survey before. Therefore the first step was to query user and subscription data for those, including the user- and device ID in order to match the usage data queried later. In contrast to the hypotheses-driven approach where extracted data rows were confronted with each other to analyze them with statistical methods, the data was combined to large CSV file, which is the default input format for machine learning algorithms. Therefore, a data expansion function was implemented which passes gathered data to the next MongoDB aggregation task and combines queried data as new column in the CSV file. A short code extract of the combiner function is shown in listing \ref{lst:csvTrainingData}. 

\begin{lstlisting}[caption={Expanding CSV data with results from MongoDB aggregation queries}, label={lst:aggregationServerCommands}]
async.waterfall([
async.apply(user.insertUserData, customerSurveyData),
subscription.insertSubscriptionData,
customerSupport.insertCustomerSupportData,
device.insertDeviceData,
shareData.insertReceivedSharesForUsers,
shareData.insertSentSharesForTrackers,
resourceData.insertResourceData,
resourceSocialData.insertResourceCommentsData,
resourceSocialData.insertResourceLikesData,
notificationData.insertNotificationsData,
geofenceData.insertGeofenceData,
geofenceReportData.insertGeofenceReportData,
posReport.insertPosReportData,
idReport.insertNumberOfDaysInUse,
serverCmd.insertServerCmdMetricsData,
appUsageData.insertAppUsageData,
hwMetricsData.insertAverageBatteryLifeTime
], cb);
\end{lstlisting}

As introduced in the code listings from section \ref{sec:extractionTool}, the async library comes in handy here again to pass an array of JSON objects along each callback in a function. This way it can be used as "hidden" input parameter in each function. Finally, the expanded JSON array gets written to a CSV file. 

\subsection{Process training data and learn a classifier}
After having prepared the raw data, the empirical work continued with the implementation of the actual prediction framework. It was decided to develop a Java application and use the open source data mining software Weka implemented and maintained by the University of Waikato in New Zealand. Based on the theory of a comprehensive KDD process as outlined by \cite{fayyad1996data} and \cite{neckel2015}, the software architecture of the framework was designed similar to a pipeline where the data flows through and gets modified at each station. The final step in this pipeline was to learn a model based on different classifier algorithms and evaluate them. The fact that survey results were collected only two weeks after a customer had activated a device, some of the attributes have invalid values among several data observations. Moreover the ratio between satisfied and dissatisfied customers is about $1.875:1$. This led to a more difficult situation and required sophisticated methods in pre-processing. Figure \ref{fig:softwareArchitecture} illustrates a sketch of the software architecture. Afterwards the individual parts shown in the figure will be explained. The thesis then dives into the implementation details with Java and Weka.

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{img/softwareArchitecture.png}
	\caption{Software architecture of the prediction framework}
	\label{fig:softwareArchitecture}
\end{figure} 

Before moving on, the essential Weka terminology should be briefly introduced to prevent any confusion afterwards. An in-memory data set in Weka is represented by the Instances object. A single data observation is called Instance and can be retrieved by calling dataSet.enumerateInstances(). Similarly, features can be accessed via dataSet.enumerateAttributes() whereby a feature in Weka terms is named Attribute. Next to a large set of different classification algorithms, Weka provides different useful filters for data pre-processing- and transformation as well as algorithms for feature selection. 

\subsubsection{FileProcessingEngine}
The task of this first component is to read the CSV outcome from the data extraction tool and convert it into the required format, namely ARFF (Attribute Relation File Format), which Weka is able to work with. The ARFF format is very similar to CSV except that it contains metadata describing the type and value range of each feature (called attribute in terms of Weka). To read in the CSV file and return so called Weka Instances, representing the data set in-memory, the ConverterUtils from Weka were used. The resulting raw data set is then passed on to the pre-processing station. Furthermore, the FileProcessingEngine implements a method to write a current data set to an ARFF file which is used to store intermediate results. Listing \ref{lst:fileProcessingEngine} shows the implementation of those methods. 

\begin{lstlisting}[caption={Implementation of FileProessingEngine}, label={lst:fileProcessingEngine}]
public static Instances readDataSetFromFile(String filename) {
	try {
		DataSource source = new DataSource(filename);
		return source.getDataSet();
	} catch (Exception e) {
		throw new RuntimeException("Data set could not be loaded from passed filename", e);
	}
}

public static void writeDataSetToFile(Instances dataSet, String filename) {
	try {
		ArffSaver arffSaver = new ArffSaver();
		arffSaver.setInstances(dataSet);
		arffSaver.setFile(new File(filename));
		arffSaver.writeBatch();
	} catch (Exception e) {
		throw new RuntimeException("Data set could not be written to file", e);
	}
}
\end{lstlisting}

\subsubsection{PreProcessingEngine}
The goal of this component is to prepare the data set in a way to build the best possible model when training a classifier at a later stage. Putting the raw data set from the FileProcessingEngine into a classification algorithm would lead to poor results as the quality of data for some features turned out to be bad. A view onto the statistics reveals that some attributes have a remarkable percentage of missing values. The features were grouped into smaller categories and are illustrated in table \ref{tab:preprocessingStatistics}.

\begin{table}[]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		\textbf{Feature}         & \textbf{Missing values} \\ \hline
		Support ticket metrics   & 68\%                    \\ \hline
		Battery lifetime & 83\%                    \\ \hline
		Command duration         & 10\%                    \\ \hline
		Command delays           & 6\%                     \\ \hline
	\end{tabular}
	\caption{Missing values for some features}
	\label{tab:preprocessingStatistics}
\end{table}

The reason why there are missing values for these features can be explained as following. As expected upfront there are not that many customers, who have contact with the company support team. Thus, 68\% of customers have a ticket count of 0 and as a consequence missing support ticket metrics. Despite that, there is data available for about $1/3$ of customers who filled in the survey which allows some separate analysis later on. Since the calculation of hardware metrics started later, there is no battery lifetime data available for most of the devices from customers which gave satisfaction feedback. Although not that extreme, also the server command duration values are missing for some data observations most likely due to the short time span between activating the tracker and filling in the survey. This indicates that some users have not yet enabled live tracking at least once a time or did not have any successful live tracking so far. From these findings it can be concluded that missing data falls into the category of NMAR (Not-Missing-At-Random) as it was explained in the background research in section \ref{sssec:explorationData}. Techniques proposed in the theory were implemented to handle those missing values. Following paragraphs take a look on the implementation details. 

\paragraph{Filter unneeded features manually}
Although the author could not exclude any particular features based on the statistical analysis from the previous chapter, the ticket metrics and the battery lifetime cannot contribute anything positive to learn a reliable model due to the large amount of missing values. They rather distort the learned model and thus have negative impact. Therefore it was decided to remove all features which exceed a defined threshold of missing values completely from the data set. With the RemoveFilter from Weka the features can be removed from the data set contained in the passed Instances object. As applying a Weka filter is a repetitive action, it was implemented as utility method in the Utils class. Code listing \ref{lst:removeUnneededFeatures} shows an example of how to remove unneeded features which have more than 25\% missing values.

\begin{lstlisting}[caption={Remove unneeded features}, label={lst:removeUnneededFeatures}]
public static Instances filterAttributesWithTooManyMissingValues(Instances dataSet) {
	return filterAttributesNotFulfillingCriteria(dataSet, attribute -> {
		int numberOfMissingValues = (int) Collections.list(dataSet.enumerateInstances())
			.stream()
			.filter(instance -> instance.isMissing(attribute))
			.count();
		float missingValuesPctg = ((float) numberOfMissingValues / dataSet.numInstances());
		return missingValuesPctg > 0.25;
	});
}
private static Instances filterAttributesNotFulfillingCriteria
(Instances dataSet, Predicate<Attribute> criteria) {
	String[] attributeNamesToRemove = Collections.list(dataSet.enumerateAttributes())
		.stream()
		.filter(criteria)
		.map(Attribute::name).toArray(String[]::new);
		
	return filterUnneededAttributes(dataSet, attributeNamesToRemove);
}
private satic Instances filterUnneededAttributes(Instances dataSet, String[] attributeNames) {
	Remove removeFilter = new Remove();
	removeFilter.setAttributeIndicesArray(getAttributeIndices(dataSet, attributeNames));
	return Utils.applyFilter(dataSet, removeFilter);
}
\end{lstlisting}

\paragraph{Filter features with no variance}
The variance of numeric features was investigated to filter extreme cases which show no variance at all. To overcome this problem those features were removed as well from the data set. The filterAttributesNotFulfillingCriteria method shown in the previous code listing was reused here with a different predicate function passed. Algorithm \ref{lst:removeAttributesNoVariance} shows how this was implemented.

\begin{lstlisting}[caption={Remove features}, label={lst:removeUnneededFeatures}]
public static Instances filterAttributesWithNoVariance(Instances dataSet) {
	return filterAttributesNotFulfillingCriteria(dataSet, attribute ->
		attribute.isNumeric() && dataSet.variance(attribute) == 0.0);
}
\end{lstlisting}

\paragraph{Imputation of missing values}
As indicated earlier in this section, 5-10 \% of the server command data collected showed missing feature values. In contrast to the cases considered before, these features could not be simply removed from the dataset as they too much important information regarding Customer Satisfaction. The categorical analysis in section \ref{sec:hypothesesDriven} showed for a small random sample a statistical significant evidence that the live tracking success rate influences customer churn. It was decided that this feature has to be kept for the data mining stage. As a result, the best possible technique was imputing the missing values accordingly. First of all, the imputation was done with the mean value of the feature under consideration. This method is available only for numeric attributes which was fine in this situation as the server command related data only consists of numeric features. Extracts of the implementation are shown in listing \ref{lst:meanImputation}.

\begin{lstlisting}[caption={Mean and mode imputation of missing values}, label={lst:meanImputation}]
private static Instances replaceMissingAttributeValues(Instances dataSet, Set<Attribute> replaceableAttributes) {
	if (!replaceableAttributes.stream().allMatch(Attribute::isNumeric)) {
		throw new RuntimeException("Not all attributes to replace with the replacement value are numeric");
	}
	Enumeration<Instance> instanceEnumeration = dataSet.enumerateInstances();
	
	while (instanceEnumeration.hasMoreElements()) {
		Instance instance = instanceEnumeration.nextElement();
		
		for (int i = 0; i < instance.numAttributes(); i++) {
			Attribute currentAttribute = instance.attribute(i);
			if (instance.isMissing(i) && replaceableAttributes.contains(currentAttribute)) {
				instance.setValue(currentAttribute, getReplacementValue(dataSet, currentAttribute));
			}
		}
	}
	return dataSet;
}

private static double getReplacementValue(Instances dataSet, Attribute attribute) {
	return Collections.list(dataSet.enumerateInstances())
		.stream()
		.filter(instance -> !instance.isMissing(attribute))
		.collect(Collectors.averagingDouble(x -> x.value(attribute)));
}
\end{lstlisting}

Next to the input data set the method takes as second parameter a set of features to replace with the mean value. First of all the method asserts whether all features to replace are numeric, otherwise an exception will be thrown. Within the subsequent while loop the features to replace are searched and the value gets substituted by a replacement value, calculated in getReplacementValue(dataSet, currentAttribute). This method iterates over all instances and calculates the arithmetic mean among the present values for the current feature.

% TODO: If I have implemented a different imputation method, add the documentation here

\paragraph{Defining the Weka class attribute}
After reading in the raw data from the CSV file, Weka does not know which of the features is used as class attribute needed to perform a classification task. The class attribute is the ground truth value for each data observation in the training set a classification algorithm uses for learning and evaluating a model. The important metrics provided by the survey results are the overall satisfaction level on a scale from 1-5 and the recommendation potential on a scale from 0-10. As the gathered experience from theory and hypotheses-driven testing at this point in time showed the difficulty of deriving relationships between certain features and Customer Satisfaction, it was decided to make the class value binary, namely satisfied or dissatisfied. With regard to business requirements it made sense to claim that below a rating of 4 customers tend to be dissatisfied while a value of 4 and 5 means that the customer tends to be satisfied. A similar machine learning approach for Customer Satisfaction, documented by \cite{meinzer2016can}, which was mentioned in the related work section did the same satisfaction level classification and thus confirms the thoughts. Therefore, in advance of setting the feature as class attribute it was normalized according to this satisfaction classification. 

As alternative method it was tried to include the recommendation score into the satisfaction classification. This consideration is supported by literature work outlined in section \ref{ssec:quantitativeMetrics}. The satisfaction score was defined as the mean value of the overall satisfaction- and the recommendation score. $6.5$ was set as threshold value to differentiate between satisfied and dissatisfied customers. After the satisfaction score had been brought into the desired format, the class attribute could be set for the current Weka Instances.

\paragraph{Normalization of numeric values}
As the numeric features are a mix of different units, like percentages for the server command metrics, absolute numbers or timestamps, a successive classification algorithm would consider some features more important as others. In order to overcome the problem of different units, a linear scaling normalization of numeric values was implemented. As a result, all values are between 0 and 1. Since Weka provides a helpful filter, the normalization can be done conveniently. Code snippet \ref{lst:normalization} shows how to apply this filter.

\begin{lstlisting}[caption={Normalization of numeric feature values}, label={lst:normalization}]
public static Instances normalizeNumericAttributeValues(Instances dataSet, double minNormalizeValue, double maxNormalizeValue) {
	Normalize normalizeFilter = new Normalize();
	normalizeFilter.setScale(maxNormalizeValue);
	normalizeFilter.setTranslation(minNormalizeValue);
	return Utils.applyFilter(dataSet, normalizeFilter);
}
\end{lstlisting}

\paragraph{Transform nominal to binary values}
Next to the numeric features, there are also attributes which are enumerations and can take on only a few values. In Weka these features are called nominal. They are not optimal since various classification algorithms can not deal with such enumerations well. The implemented solution transforms the nominal features into binary ones by splitting the given feature into as many separate features as values are possible. For each data observation either 0 or 1 gets associated for the particular transformed feature. 
Weka again provides a filter which automatically creates new features based on existing ones. 
Code listing \ref{lst:nominalToBinary} illustrates the implemented transformation.

\begin{lstlisting}[caption={Transformation of nominal- to binary features}, label={lst:nominalToBinary}]
public static Instances transformNominalToBinaryAttributes(Instances dataSet) {
	NominalToBinary nominalToBinaryFilter = new NominalToBinary();
	nominalToBinaryFilter.setTransformAllValues(false);
	nominalToBinaryFilter.setBinaryAttributesNominal(true);
	return Utils.applyFilter(dataSet, nominalToBinaryFilter);
}
\end{lstlisting}

\paragraph{Dealing with class imbalance}
Depending on the dataset under consideration an imbalance between the ground truth values is a quite common scenario one has to deal with in classification tasks. A typical example related to class imbalance is the prediction of medical diagnosis cases where the clear majority in the sample is tested negative while a small minority is infected. Before having a look into the data details, the author did not pay any attention to the class distribution in particular but after spot checking a few promising machine learning algorithms on the pre-processed but yet imbalanced dataset, it turned out that the results are very bad and not useful at all. The class distribution of the dataset is shown in a simple bar chart illustrated in figure \ref{fig:classDistribution}. 

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{img/classImbalance.png}
	\caption{Bar chart illustration of class imbalance problem in customer dataset}
	\label{fig:classDistribution}
\end{figure} 

However, as indicated in the background research, different techniques to successfully deal with such imbalances have been researched. In the implementation part focus was paid in particular to the sampling methods. The initial attempt was to perform a simple undersampling on available training data by randomly taking only as many data observations from the satisfied customers to achieve a desired equality between the number of those observations and the dissatisfied ones. In order to give the method a better applicability in the framework, a factor indicating the percentage of minority- relative to majority class observations can be passed. The implementation is shown in listing \ref{lst:simpleUndersampling}. 

\begin{lstlisting}[caption={Implementation of a random undersampling method}, label={lst:simpleUndersampling}]
public static Instances simpleUnderSampling(Instances dataSet, double minorityClassFactor) {
	List<Instance> instancesMajorityClass = filterInstancesByClassValue(dataSet,1).collect(Collectors.toList());
	int numRequiredMajorityInstances = (int) (instancesMajorityClass.size() * minorityClassFactor);
	
	Collections.shuffle(instancesMajorityClass);
	instancesMajorityClass = instancesMajorityClass.subList(0, numRequiredMajorityInstances);
	dataSet.removeIf(instance -> instance.classValue() == 1);
	dataSet.addAll(instancesMajorityClass);

	return dataSet;
}
private static Stream<Instance> filterInstancesByClassValue(Instances dataSet, double classValue) {
	return Collections
		.list(dataSet.enumerateInstances())
		.stream()
		.filter(instance -> instance.classValue() == classValue);
}
\end{lstlisting}

Although the class imbalance is not as extreme as in other use cases, like data dealing with the prediction of a disease, the random undersampling approach can be considered as an rather trivial algorithm to resolve class imbalance. Due to the reason that the top-down analysis showed that the quality of the data with regard to Customer Satisfaction prediction is not very good and the available dataset is not that huge, throwing away a few hundred observations could remove valuable information for the classification task. Therefore this thesis searched for an oversampling method as alternative. As outlined in respective literature, the difficulty in oversampling is the generation of new observations. Although a combination of simple undersampling and duplication of observations belonging to the minority usually leads to improved classification accuracy on a validation set, it is often affected by overfitting to the learned model. Thus, the accuracy on real test set turns out to be completely different in contrast to the training phase. As a result of literature research, the focus was put on the implementation of the SMOTE algorithm to generate new observations fitting to a realistic distribution of provided data. The implementation of SMOTE is illustrated in pseudo code in listing \ref{lst:smote}. 

\begin{lstlisting}[caption={SMOTE algorithm in pseudo code}, label={lst:smote}]
Instances smote(dataSet, oversamplingPctg, k) {
	if (oversamplingPctg < 100) {
		chooseRandomSamples(dataSet, oversamplingPctg)
	}
	for i <- 0..dataSet.length {
		nearestNeighbors = chooseNearestNeighbors(dataSet[i], dataSet, k)
		syntheticObservations.add(generateSyntheticObservations(dataSet[i], nearestNeighbors))
	}
	return merge(dataSet, syntheticObservations)
}

Instances chooseNearestNeighbors(dataSet[i], dataSet, k) {
	nearestNeighbors = []
	for i <- 0..k {
		nearestNeighbors[i] = euclideanDistance(dataSet[i], dataSet)
	}
	return nearestNeighborIndices
}

Instances generateSyntheticObservations(dataSet, observation, nearestNeighborIndices, k) {
	syntheticSamples = []
	for i <- 0..k {
		for j <- 0..dataSet[i].attributesNr {
			diff = observation[i] - dataSet[nearestNeighborIndices][i]
			syntheticSamples[i][j] = observation[i] + random(0, 1) * diff
		}
	}
	return syntheticSamples
}
\end{lstlisting}

\subsubsection{Feature Engineering}
The first step in limiting the feature space was done manually based on the fit of data sources to the proposed Customer Satisfaction model. Since there are still a lot of features in preprocessed data set, the author decided to implement an automatic feature selection algorithm to find and skip redundant ones which potentially even diminish classification accuracy. The first algorithm is based on the intra-inter-correlation feature selection technique introduced in section \ref{sssec:featureEngineering}. Therefore an abstract class called AttributeSelectionStrategy was implemented which generates a list of possible feature subsets and returns the best one depending on the concrete selection algorithm. An extract of this class is shown in code listing \ref{lst:attributeSelectionStrategy}.

\begin{lstlisting}[caption={AttributeSelectionStrategy}, label={lst:attributeSelectionStrategy}]
@Override
public Set<Attribute> getBestAttributeSet() {
	List<Set<Attribute>> attributeSubSets = generateAttributeSubsets();
	return getBestAttributeSubSet(attributeSubSets);
}

List<Set<Attribute>> generateAttributeSubsets() {
	List<Set<Attribute>> attributeSubSets = new ArrayList<>();
	ThreadLocalRandom random = ThreadLocalRandom.current();
	
	for (int i = 0; i < maxIterations; i++) {
		Set<Attribute> currentAttributeSet = new HashSet<>();
		
		for (int j = 0; j <= random.nextInt(minSetSize, maxSetSize); j++) {
			int randomAttributeIndex = random.nextInt(0, getDataSet().numAttributes());
			currentAttributeSet.add(getDataSet().attribute(randomAttributeIndex));
		}
		attributeSubSets.add(currentAttributeSet);
	}
	return attributeSubSets;
}

abstract Set<Attribute> getBestAttributeSubSet(List<Set<Attribute>> attributeSubSets);
\end{lstlisting}

The method getBestAttributeSubSet is implemented in a concrete class inheriting the AttributeSelectionStrategy and provides an implementation for the selection of the best possible attribute subset. In terms of correlation based feature selection, first each all subsets are iterated and for each one the overall correlation, consisting of inter- and intra coefficient, as outlined in section \ref{sssec:featureEngineering} gets calculated. The subsets are then sorted depending on the correlation coefficient in reverse order and the first one picked. Subsequently the feature engineering filters the identified features within the best subset by applying the Remove filter from Weka. 

As indicated in the theory of feature engineering, a powerful technique is the learner based feature selection. Although it has the disadvantage of higher runtime complexity, especially with an increased number of features, the author decided to give it a try and check whether results can be improved. For implementation the WrapperSubsetEval class provided by Weka was used. For the learner evaluation typically a fast classifier should be used. As decision trees are proven to be fast, I selected the C4.5 Java implementation called J48 as base learner. The last attribute to be set in the AttributeSelection filter is the search method indicating when the attribute selection has to be terminated. Based on research results of \cite{aggarwal2013performance} it was decided to use the BestFirst search method. Due to the limitations of this thesis no detailed research comparing different search methods for feature selection was conducted. After applying the configured AttributeSelection filter, Weka returns the data set with reduced feature dimensions selected.

\subsubsection{ClassificationEngine}
The final part in the Customer Satisfaction prediction framework was the training of classification algorithms on the preprocessed, transformed and feature engineered data set. As revealed in literature research pointed out in section \ref{ssec:bottomUp}, there is no ultimate classifier for such a type of data set. Although Weka provides a GUI called Weka Explorer which allows fast application of classifiers on a data set, it is only intended for manual experiments. Thus, the Explorer was only used a few times in the beginning to get a feeling on the development of prediction accuracy and getting an overview on the provided evaluation measures. Instead, the goal was to implement a program, which gets passed different versions of preprocessed data sets and a set of classification algorithms as input, and finds the best combination of data set and classifier based on a representative measure. 

In order to realize this intention, a new class called DataSetCreator was introduced. This class first of all is responsible for configuring necessary parameters regarding preprocessing, data transformation and feature engineering. The main functionality of the DataSetCreator is to use a passed configuration and generate the desired data sets as output. A code extract of this class is illustrated in listing \ref{lst:dataSetCreator}. 

\begin{lstlisting}[caption={DataSetCreator}, label={lst:dataSetCreator}]
private Instances createPreprocessedDataSet(Instances dataSet, boolean filterTooManyMissing,
boolean replaceMissingAttrValues, boolean markAllMissingNumericValues,
boolean replaceAllMissingAttrValues,
AbstractMap.SimpleEntry<Boolean, Integer> discretizeConfig,
SamplingConfig simpleUndersampling, SamplingConfig simpleOversampling,
SmoteSamplingConfig smoteOversampling) {
	System.out.println("Starting to create a preprocessed data set");
	
	if (filterTooManyMissing) {
		dataSet = PreProcessingEngine.filterAttributesWithTooManyMissingValues(dataSet);
	}
	if (replaceAllMissingAttrValues) {
		dataSet = PreProcessingEngine.replaceAllMissingAttributeValues(dataSet);
	}
	if (simpleUndersampling.isEnabled()) {
		dataSet = PreProcessingEngine.simpleUnderSampling(dataSet, simpleUndersampling.getPercentageMinorityClass());
	}
	if (smoteOversampling.isEnabled()) {
		dataSet = PreProcessingEngine.smoteOversampling(dataSet, smoteOversampling.getPercentageMinorityClass(), smoteOversampling.getNumNeighbors());
	}
	// more code ...
	return dataSet;
}
\end{lstlisting}

Different preprocessing combinations were defined by the author. For all generated datasets the unneeded attributes which were mentioned in the PreprocessingEngine get removed upfront and nominal values transformed to binary ones as classification algorithms cannot deal well with pure nominal ones. A selection of data sets was done by using combinations of following configurations:

\begin{itemize}
	\item Replace missing numeric values with the mean value of a particular feature 
	\item Replace binary features with the mode value of a particular feature
	\item Discretize numeric values by putting them into 10 bins. (Due to the amount of configuration combinations and resulting time constraints, no further experimenting regarding the number of discretization bins was conducted)
	\item Undersampling the training set of satisfied customers by applying the random undersampling introduced earlier in this chapter.
	\item Oversampling the training set of dissatisfied customers by duplicating randomly chosen observations.
	\item SMOTE oversampling the training set of dissatisfied customers.
\end{itemize}

Regarding the sampling methods, the minority respectively majority class was sampled on a range starting with 70\% and increased in 5\% steps. As a result of generating data sets by these different configurations, a total number of xx data sets was created. 

The next challenge now deals with the processing of these data sets by classification algorithms. Therefore a class named ClassifiersCreator was implemented. The main task of this class is to instantiate different classifiers, which were promised to perform well on similar Customer Satisfaction related problems introduced in literature. Moreover as baseline the author decided to use an algorithm called ZeroR in Weka which only relies on the class values and only predicts the majority class correctly. This classifier is not considered as relevant for the thesis but should only be used as benchmark for comparison. The actual classification algorithms discussed in more detail in section \ref{ssec:classificationAlgorithms} were included in the framework. 

Due to the reason that dissatisfied customers are more important to be reliably detected in order to tackle their problems early enough, the author decided to experiment with penalties for misclassification. Therefore Weka provides an abstract class called CostSensitiveClassifier. This class allows to pass a penalty matrix where the author specified a value ranging between 1.1 and 2.0 incremented by 0.1. The remaining step after initializing the penalty matrix is to wrap the base classifier. Having instantiated a set of Weka Instances, representing the data sets to be classified, and a set of Weka classifiers, the models can be built and evaluated. 

Hence, the main method in the ClassificationEngine iterates over the data sets and trains every selected classifier on each of them. Based on literature research the preferred evaluation method is a 10-fold cross validation, which was used in the implementation of this prediction framework as well. Therefore the Weka Evaluation class with the method crossValidateModel was used. Evaluated classifiers get compared first by the AUC value, representing the area under the ROC calculated by integrating the respective function of the curve. As second measurement it was decided to consider the percentage of correctly classified instances as the preprocessing actively tackled the class imbalance issue. These values are provided by the Evaluation class after cross validating the learned model. If both values are high enough, the combination of configured data set and classifier gets stored in memory. After completing evaluation iterations, both the best- dataset and classifier gets stored with all details in separate files on disk for later analysis of the results (see chapter \ref{ch:evaluation}). The quintessence of the ClassificationEngine is illustrated in code extract \ref{lst:classificationEngine}. 

\begin{lstlisting}[caption={Applying configured classifiers on preprocessed data sets}, label={lst:classificationEngine}]
public void classifyDataSets() {
	double bestPctCorrect = 0.0;
	double bestAuc = 0.0;
	AbstractMap.SimpleEntry<Instances, Classifier> bestCombination = null;
	Evaluation bestEvaluation = null;
	long startTime = new Date().getTime();
	int datasetNr = 1;
	
	for (Instances dataSet : dataSets) {
		for (Classifier classifier : classifiers) {
			try {
				classifier.buildClassifier(dataSet);
				Evaluation evaluation = new Evaluation(dataSet);
				evaluation.crossValidateModel(classifier, dataSet, NUM_FOLDS, new Random(1));
	
				double auc = evaluation.weightedAreaUnderROC();
				double pctCorrect = evaluation.pctCorrect();
				
				if (bestCombination == null || (pctCorrect > bestPctCorrect && auc > bestAuc)) {
					bestCombination = new AbstractMap.SimpleEntry<>(dataSet, classifier);
					bestPctCorrect = evaluation.pctCorrect();
					bestAuc = auc;
					bestEvaluation = evaluation;
				}
			} catch (Exception e) {
				System.out.println("Applying classifier " + classifier.toString() + " to dataset with relation name" +
				dataSet.relationName() + " was not successful.");
			}
		}
		System.out.println("Classifier evaluation for data set " + datasetNr + " took " +
		Utils.timeDifferenceToString(new Date().getTime() - startTime));
		datasetNr++;
	}
	System.out.println("Wohoo! All data sets evaluated!");
	saveBestCombination(bestCombination, bestEvaluation);
}
\end{lstlisting}

This chapter provided a thorough view onto the functionality and implementation of the automatic Customer Satisfaction prediction framework based on a data-driven approach. The outcome will be illustrated and discussed in the following chapter.







 



